{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Mohamed\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import pyarabic.normalize as Normalize\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import ISRIStemmer\n",
    "from transformers import MarianMTModel, MarianTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emojis(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                                    u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                                    u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                                    u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                                    u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                                    u\"\\U00002702-\\U000027B0\"\n",
    "                                    u\"\\U000024C2-\\U0001F251\"\n",
    "                                    u\"\\U0001F90C-\\U0001F93A\"  # Supplemental Symbols\n",
    "                                    u\"\\U0001F93C-\\U0001F945\"  # and\n",
    "                                    u\"\\U0001F947-\\U0001F9FF\"  # Pictographs\n",
    "                                \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeConsecutiveDuplicates(text):\n",
    "    # Replace any group of two or more consecutive characters with just one\n",
    "    #clean = re.sub(r'(\\S)(\\1+)', r'\\1', text, flags=re.UNICODE)\n",
    "\n",
    "    clean = re.sub(r'(\\S)(\\1{2,})', r'\\1', text, flags=re.UNICODE)\n",
    "    #This one only replaces it if there are more than two duplicates. For example, الله has 2 لs but we don't want it removed\n",
    "\n",
    "    return clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeEnglish(text):\n",
    "    return re.sub(r\"[A-Za-z0-9]+\",\"\",text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizeArabic(text):\n",
    "    \"\"\"\n",
    "    This function takes an Arabic word as input and returns its lemma using NLTK's ISRI stemmer\n",
    "    \"\"\"\n",
    "    # Create an instance of the ISRI stemmer\n",
    "    stemmer = ISRIStemmer()\n",
    "    # Apply the stemmer to the word\n",
    "    lemma = stemmer.stem(text)\n",
    "    return lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeStopwords(text):\n",
    "    # Tokenize the text into wordsz\n",
    "    words = nltk.word_tokenize(text)\n",
    "    # Get the Arabic stop words from NLTK\n",
    "    stop_words = set(stopwords.words('arabic'))\n",
    "    # Remove the stop words from the list of words\n",
    "    words_filtered = [word for word in words if word.lower() not in stop_words]\n",
    "    # Join the words back into a string\n",
    "    clean = ' '.join(words_filtered)\n",
    "    return clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removePunctuation(text):\n",
    "    # Define the Arabic punctuation regex pattern\n",
    "    arabicPunctPattern = r'[؀-؃؆-؊،؍؛؞]'\n",
    "    engPunctPattern = r'[.,;''`~:\"]'\n",
    "    # Use re.sub to replace all occurrences of Arabic punctuation with an empty string\n",
    "    clean = re.sub(arabicPunctPattern + '|' + engPunctPattern, '', text)\n",
    "    return clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanData(dataset):\n",
    "    dataset = dataset.drop_duplicates(subset=[\"tweet\"])\n",
    "    dataset = dataset.dropna()\n",
    "    dataset = dataset.reset_index(drop=True)\n",
    "\n",
    "    for index, tweet in enumerate(dataset[\"tweet\"].tolist()):\n",
    "        #standard tweet cleaning\n",
    "        clean = re.sub(r\"(http[s]?\\://\\S+)|([\\[\\(].*[\\)\\]])|([#@]\\S+)|\\n\", \"\", tweet)\n",
    "        \n",
    "        #Test to see if they're useful or not\n",
    "        clean = remove_emojis(clean)\n",
    "        clean = removeConsecutiveDuplicates(clean)\n",
    "\n",
    "        # mandatory arabic preprocessing\n",
    "        clean = Normalize.normalize_searchtext(clean)\n",
    "        clean = removeEnglish(clean)\n",
    "        clean = lemmatizeArabic(clean)\n",
    "        clean = removeStopwords(clean)\n",
    "        clean = removePunctuation(clean)\n",
    "\n",
    "        # clean = tokenizeArabic(clean)\n",
    "        dataset.loc[index, \"tweet\"] = clean # replace the old values with the cleaned one.\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_batch_texts(language_code, batch_texts):\n",
    "    formated_bach = [\">>{}<< {}\".format(language_code, text) for text in batch_texts]\n",
    "    return formated_bach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "englishModelName = \"Helsinki-NLP/opus-mt-ar-en\"\n",
    "arabicModelName = \"Helsinki-NLP/opus-mt-en-ar\"\n",
    "\n",
    "englishModeltkn = MarianTokenizer.from_pretrained(englishModelName)\n",
    "arabicModeltkn = MarianTokenizer.from_pretrained(arabicModelName)\n",
    "\n",
    "englishModel = MarianMTModel.from_pretrained(englishModelName)\n",
    "arabicModel = MarianMTModel.from_pretrained(arabicModelName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_translation(batch_texts, model, tokenizer, language=\"en\"):\n",
    "    # Prepare the text data into appropriate format for the model\n",
    "    formated_batch_texts = format_batch_texts(language, batch_texts)\n",
    "    \n",
    "    # Generate translation using model\n",
    "    translated = model.generate(**tokenizer(formated_batch_texts, return_tensors=\"pt\", padding=True))\n",
    "\n",
    "    # Convert the generated tokens indices back into text\n",
    "    translated_texts = [tokenizer.decode(t, skip_special_tokens=True) for t in translated]\n",
    "    \n",
    "    return translated_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_synonyms(word):\n",
    "    \"\"\"\n",
    "    Get synonyms of a word\n",
    "    \"\"\"\n",
    "    synonyms = set()\n",
    "    \n",
    "    for syn in wordnet.synsets(word): \n",
    "        for l in syn.lemmas(): \n",
    "            synonym = l.name().replace(\"_\", \" \").replace(\"-\", \" \").lower()\n",
    "            synonym = \"\".join([char for char in synonym if char in ' qwertyuiopasdfghjklzxcvbnm'])\n",
    "            synonyms.add(synonym) \n",
    "    \n",
    "    if word in synonyms:\n",
    "        synonyms.remove(word)\n",
    "    \n",
    "    return list(synonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synonym_replacement(words, n):\n",
    "    \n",
    "    words = nltk.word_tokenize(words)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    new_words = words.copy()\n",
    "    \n",
    "    random_word_list = list(set([word for word in words if word not in stop_words]))\n",
    "    random.shuffle(random_word_list)\n",
    "    num_replaced = 0\n",
    "    \n",
    "    for random_word in random_word_list:\n",
    "        synonyms = get_synonyms(random_word)\n",
    "        \n",
    "        if len(synonyms) >= 1:\n",
    "            synonym = random.choice(list(synonyms))\n",
    "            new_words = [synonym if word == random_word else word for word in new_words]\n",
    "            num_replaced += 1\n",
    "        \n",
    "        if num_replaced >= n: #only replace up to n words\n",
    "            break\n",
    "\n",
    "    sentence = ' '.join(new_words)\n",
    "\n",
    "    return sentence\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataAugmentation(dataset):\n",
    "    augDataset = pd.DataFrame(columns=[\"tweet\", \"dialect\", \"sentiment\", \"sarcasm\"])\n",
    "\n",
    "    sarcasmTweets = dataset[dataset.sarcasm == 1][\"tweet\"].tolist()\n",
    "    sarcasmTweets_dialect = dataset[dataset.sarcasm ==1][\"dialect\"].tolist()\n",
    "    sarcasmTweets_sentiment = dataset[dataset.sarcasm ==1][\"sentiment\"].tolist()\n",
    "\n",
    "\n",
    "    for index in range(len(sarcasmTweets)):\n",
    "        englishVersion = perform_translation([sarcasmTweets[index]], englishModel, englishModeltkn, \"en\")\n",
    "        englishVersion = synonym_replacement(\" \".join(englishVersion), len(englishVersion))\n",
    "        try:\n",
    "            arabicVersion = perform_translation([englishVersion], arabicModel, arabicModeltkn, \"ar\")\n",
    "        except:\n",
    "            print(arabicVersion)\n",
    "\n",
    "        augDataset.loc[len(augDataset.index)] = [\n",
    "                                                    \" \".join(arabicVersion),\n",
    "                                                    sarcasmTweets_dialect[index],\n",
    "                                                    sarcasmTweets_sentiment[index],\n",
    "                                                    True\n",
    "                                                ]\n",
    "\n",
    "    return augDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcessData(dataset):\n",
    "\n",
    "    data = cleanData(dataset.copy(deep=True))\n",
    "    print(\"\\n-------        cleanData Done!        -------\\n\")\n",
    "\n",
    "    data = dataAugmentation(data.copy(deep=True))\n",
    "    print(\"\\n---------- dataAugmentation Done! ----------\\n\")\n",
    "\n",
    "    data = cleanData(data.copy(deep=True))\n",
    "    print(\"\\n-------        cleanData Done!        -------\\n\")  \n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(r\"https://raw.githubusercontent.com/iabufarha/ArSarcasm-v2/main/ArSarcasm-v2/training_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "startTime = time.time()\n",
    "data = preProcessData(dataset.copy(deep=True))\n",
    "endTime = time.time()\n",
    "\n",
    "executionTime = endTime - startTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"execution time: {executionTime}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synReplacement_data = data.copy(deep=True)\n",
    "synReplacement_data = synReplacement_data[[\"tweet\", \"sentiment\", \"dialect\", \"sarcasm\"]]\n",
    "\n",
    "backTrans_data = pd.read_csv(\"afterBackTrans.csv\")\n",
    "\n",
    "result = pd.concat([synReplacement_data, backTrans_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 16491 entries, 0 to 16490\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   tweet      16491 non-null  object\n",
      " 1   sentiment  16491 non-null  object\n",
      " 2   dialect    16491 non-null  object\n",
      " 3   sarcasm    16491 non-null  bool  \n",
      "dtypes: bool(1), object(3)\n",
      "memory usage: 402.7+ KB\n"
     ]
    }
   ],
   "source": [
    "result = result.sample(frac = 1)\n",
    "result = result.drop_duplicates(subset=[\"tweet\"])\n",
    "result = result.dropna()\n",
    "result = result.reset_index(drop=True)\n",
    "\n",
    "result.to_csv(\"synonym_replacement.csv\", index=False)\n",
    "result.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 18537 entries, 0 to 18536\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   tweet      18537 non-null  object\n",
      " 1   sentiment  18537 non-null  object\n",
      " 2   dialect    18537 non-null  object\n",
      " 3   sarcasm    18537 non-null  bool  \n",
      "dtypes: bool(1), object(3)\n",
      "memory usage: 452.7+ KB\n"
     ]
    }
   ],
   "source": [
    "datasetA = pd.read_csv(\"synonym_replacement.csv\")\n",
    "datasetB = pd.read_csv(\"dataset_GPT.csv\")\n",
    "\n",
    "dataset = pd.concat([datasetA, datasetB])\n",
    "dataset = dataset.sample(frac = 1)\n",
    "dataset = dataset.drop_duplicates(subset=[\"tweet\"])\n",
    "dataset = dataset.dropna()\n",
    "dataset = dataset.reset_index(drop = True)\n",
    "\n",
    "dataset.to_csv(\"Total Dataset.csv\", index=False)\n",
    "dataset.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
