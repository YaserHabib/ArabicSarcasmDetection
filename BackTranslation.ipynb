{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Mohamed\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import time\n",
    "import pandas as pd\n",
    "import pyarabic.normalize as Normalize\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import ISRIStemmer\n",
    "from transformers import MarianMTModel, MarianTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emojis(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                                    u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                                    u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                                    u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                                    u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                                    u\"\\U00002702-\\U000027B0\"\n",
    "                                    u\"\\U000024C2-\\U0001F251\"\n",
    "                                    u\"\\U0001F90C-\\U0001F93A\"  # Supplemental Symbols\n",
    "                                    u\"\\U0001F93C-\\U0001F945\"  # and\n",
    "                                    u\"\\U0001F947-\\U0001F9FF\"  # Pictographs\n",
    "                                \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeConsecutiveDuplicates(text):\n",
    "    # Replace any group of two or more consecutive characters with just one\n",
    "    #clean = re.sub(r'(\\S)(\\1+)', r'\\1', text, flags=re.UNICODE)\n",
    "\n",
    "    clean = re.sub(r'(\\S)(\\1{2,})', r'\\1', text, flags=re.UNICODE)\n",
    "    #This one only replaces it if there are more than two duplicates. For example, الله has 2 لs but we don't want it removed\n",
    "\n",
    "    return clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeEnglish(text):\n",
    "    return re.sub(r\"[A-Za-z0-9]+\",\"\",text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizeArabic(text):\n",
    "    \"\"\"\n",
    "    This function takes an Arabic word as input and returns its lemma using NLTK's ISRI stemmer\n",
    "    \"\"\"\n",
    "    # Create an instance of the ISRI stemmer\n",
    "    stemmer = ISRIStemmer()\n",
    "    # Apply the stemmer to the word\n",
    "    lemma = stemmer.stem(text)\n",
    "    return lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeStopwords(text):\n",
    "    # Tokenize the text into wordsz\n",
    "    words = nltk.word_tokenize(text)\n",
    "    # Get the Arabic stop words from NLTK\n",
    "    stop_words = set(stopwords.words('arabic'))\n",
    "    # Remove the stop words from the list of words\n",
    "    words_filtered = [word for word in words if word.lower() not in stop_words]\n",
    "    # Join the words back into a string\n",
    "    clean = ' '.join(words_filtered)\n",
    "    return clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removePunctuation(text):\n",
    "    # Define the Arabic punctuation regex pattern\n",
    "    arabicPunctPattern = r'[؀-؃؆-؊،؍؛؞]'\n",
    "    engPunctPattern = r'[.,;''`~:\"]'\n",
    "    # Use re.sub to replace all occurrences of Arabic punctuation with an empty string\n",
    "    clean = re.sub(arabicPunctPattern + '|' + engPunctPattern, '', text)\n",
    "    return clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanData(dataset):\n",
    "    dataset = dataset.drop_duplicates(subset=[\"tweet\"])\n",
    "    dataset = dataset.dropna()\n",
    "    dataset = dataset.reset_index(drop=True)\n",
    "\n",
    "    for index, tweet in enumerate(dataset[\"tweet\"].tolist()):\n",
    "        #standard tweet cleaning\n",
    "        clean = re.sub(r\"(http[s]?\\://\\S+)|([\\[\\(].*[\\)\\]])|([#@]\\S+)|\\n\", \"\", tweet)\n",
    "        \n",
    "        #Test to see if they're useful or not\n",
    "        clean = remove_emojis(clean)\n",
    "        clean = removeConsecutiveDuplicates(clean)\n",
    "\n",
    "        # mandatory arabic preprocessing\n",
    "        clean = Normalize.normalize_searchtext(clean)\n",
    "        clean = removeEnglish(clean)\n",
    "        clean = lemmatizeArabic(clean)\n",
    "        clean = removeStopwords(clean)\n",
    "        clean = removePunctuation(clean)\n",
    "\n",
    "        # clean = tokenizeArabic(clean)\n",
    "        dataset.loc[index, \"tweet\"] = clean # replace the old values with the cleaned one.\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_batch_texts(language_code, batch_texts):\n",
    "    formated_bach = [\">>{}<< {}\".format(language_code, text) for text in batch_texts]\n",
    "    return formated_bach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "englishModelName = \"Helsinki-NLP/opus-mt-ar-en\"\n",
    "# frenchModelName = \"Helsinki-NLP/opus-mt-en-fr\"\n",
    "arabicModelName = \"Helsinki-NLP/opus-mt-en-ar\"\n",
    "\n",
    "englishModeltkn = MarianTokenizer.from_pretrained(englishModelName)\n",
    "# frenchModeltkn = MarianTokenizer.from_pretrained(frenchModelName)\n",
    "arabicModeltkn = MarianTokenizer.from_pretrained(arabicModelName)\n",
    "\n",
    "englishModel = MarianMTModel.from_pretrained(englishModelName)\n",
    "# frenchModel = MarianMTModel.from_pretrained(frenchModelName)\n",
    "arabicModel = MarianMTModel.from_pretrained(arabicModelName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_translation(batch_texts, model, tokenizer, language=\"en\"):\n",
    "    # Prepare the text data into appropriate format for the model\n",
    "    formated_batch_texts = format_batch_texts(language, batch_texts)\n",
    "    \n",
    "    # Generate translation using model\n",
    "    translated = model.generate(**tokenizer(formated_batch_texts, return_tensors=\"pt\", padding=True))\n",
    "\n",
    "    # Convert the generated tokens indices back into text\n",
    "    translated_texts = [tokenizer.decode(t, skip_special_tokens=True) for t in translated]\n",
    "    \n",
    "    return translated_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataAugmentation(dataset):\n",
    "    sarcasmTweets = dataset[dataset.sarcasm == 1][\"tweet\"].tolist()\n",
    "    sarcasmTweets_dialect = dataset[dataset.sarcasm ==1][\"dialect\"].tolist()\n",
    "    sarcasmTweets_sentiment = dataset[dataset.sarcasm ==1][\"sentiment\"].tolist()\n",
    "\n",
    "\n",
    "    for index in range(len(sarcasmTweets)):\n",
    "        englishVersion = perform_translation([sarcasmTweets[index]], englishModel, englishModeltkn, \"en\")\n",
    "        arabicVersion = perform_translation(englishVersion, arabicModel, arabicModeltkn, \"ar\")\n",
    "\n",
    "        newLocation = len(dataset)\n",
    "\n",
    "        dataset.at[newLocation, \"tweet\"] = arabicVersion\n",
    "        dataset.at[newLocation, \"dialect\"] = sarcasmTweets_dialect[index]\n",
    "        dataset.at[newLocation, \"sentiment\"] = sarcasmTweets_sentiment[index]\n",
    "\n",
    "        print(arabicVersion)\n",
    "\n",
    "    dataset = dataset[\"sarcasm\"].fillna(True)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcessData(dataset):\n",
    "\n",
    "    data = cleanData(dataset.copy(deep=True))\n",
    "    print(\"\\n-------        cleanData Done!        -------\\n\")\n",
    "\n",
    "    data = dataAugmentation(data.copy(deep=True))\n",
    "    print(\"\\n---------- dataAugmentation Done! ----------\\n\")\n",
    "\n",
    "    data = cleanData(data.copy(deep=True))\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(r\"https://raw.githubusercontent.com/iabufarha/ArSarcasm-v2/main/ArSarcasm-v2/training_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "startTime = time.time()\n",
    "data = preProcessData(dataset.copy(deep=True))\n",
    "endTime = time.time()\n",
    "\n",
    "executionTime = endTime - startTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"execution time: {executionTime}s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
