# -*- coding: utf-8 -*-
"""Sarcasm_Model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fA0Z9TXYuseG3lCsrfOx9_6yrY8luSDj
"""

!pip install pyarabic
!pip install transformers
!pip install sentencepiece

import re
import pandas as pd
import nltk
from nltk.corpus import stopwords
from nltk.stem import ISRIStemmer
from nltk.tokenize import word_tokenize
from nltk import ngrams

import seaborn as sns

import pyarabic.araby as araby
import pyarabic.normalize as Normalize
from google.colab import files
from transformers import MarianMTModel, MarianTokenizer


nltk.download('punkt')  # download punkt tokenizer if not already downloaded
nltk.download('stopwords') # download stopwords if not already downloaded
nltk.download('averaged_perceptron_tagger')

dataset = pd.read_csv(r"https://raw.githubusercontent.com/iabufarha/ArSarcasm-v2/main/ArSarcasm-v2/training_data.csv")
tweetColumn = dataset[['tweet']]

def remove_emojis(text):
    emoji_pattern = re.compile("["
                                    u"\U0001F600-\U0001F64F"  # emoticons
                                    u"\U0001F300-\U0001F5FF"  # symbols & pictographs
                                    u"\U0001F680-\U0001F6FF"  # transport & map symbols
                                    u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
                                    u"\U00002702-\U000027B0"
                                    u"\U000024C2-\U0001F251"
                                    u"\U0001F90C-\U0001F93A"  # Supplemental Symbols
                                    u"\U0001F93C-\U0001F945"  # and
                                    u"\U0001F947-\U0001F9FF"  # Pictographs
                                "]+", flags=re.UNICODE)
    return emoji_pattern.sub(r'', text)

def removeEnglish(text):
    return re.sub(r"[A-Za-z0-9]+","",text)

def removeConsecutiveDuplicates(text):
    # Replace any group of two or more consecutive characters with just one
    #clean = re.sub(r'(\S)(\1+)', r'\1', text, flags=re.UNICODE)

    clean = re.sub(r'(\S)(\1{2,})', r'\1', text, flags=re.UNICODE)
    #This one only replaces it if there are more than two duplicates. For example, الله has 2 لs but we don't want it removed

    return clean

def lemmatizeArabic(text):
    """
    This function takes an Arabic word as input and returns its lemma using NLTK's ISRI stemmer
    """
    # Create an instance of the ISRI stemmer
    stemmer = ISRIStemmer()
    # Apply the stemmer to the word
    lemma = stemmer.stem(text)
    return lemma

def tokenizeArabic(text):
    # Tokenize the text using the word_tokenize method and return the list of tokenized words
    return word_tokenize(text)

def removeStopwords(text):
    # Tokenize the text into wordsz
    words = nltk.word_tokenize(text)
    # Get the Arabic stop words from NLTK
    stop_words = set(stopwords.words('arabic'))
    # Remove the stop words from the list of words
    words_filtered = [word for word in words if word.lower() not in stop_words]
    # Join the words back into a string
    clean = ' '.join(words_filtered)
    return clean

def removePunctuation(text):
    # Define the Arabic punctuation regex pattern
    arabicPunctPattern = r'[؀-؃؆-؊،؍؛؞]'
    engPunctPattern = r'[.,;''`~:"]'
    # Use re.sub to replace all occurrences of Arabic punctuation with an empty string
    clean = re.sub(arabicPunctPattern + '|' + engPunctPattern, '', text)
    return clean

def extractSarcasticFeatures(tweet):
    """
    Extract features from an Arabic tweet for sarcasm detection.

    Args:
        tweet (str): The Arabic tweet to extract features from.

    Returns:
        dict: A dictionary of features extracted from the tweet for now.

    IDEA: Put this in cleanData and add another return there for this dict. Remove unwanted features and keep wanted ones
    """
    # Define a list of Arabic stop words
    stop_words = set(nltk.corpus.stopwords.words('arabic'))

    # Tokenize the tweet into words
    words = word_tokenize(tweet)

    # Calculate the ratio of stop words in the tweet
    stop_word_count = sum(1 for word in words if word in stop_words)

    # Count the number of exclamation and question marks in the tweet
    exclamation_count = len(re.findall(r'!', tweet))
    question_count = len(re.findall(r'\?', tweet))

    # Extract bigram features from the tweet
    bigrams = list(ngrams(words, 2))
    bigram_freq = nltk.FreqDist(bigrams)
    top_bigrams = tuple(bigram_freq.most_common(5))

    # Extract trigram features from the tweet
    trigrams = list(ngrams(words, 3))
    trigram_freq = nltk.FreqDist(trigrams)
    top_trigrams = tuple(trigram_freq.most_common(5))

    # Count the number of Arabic emojis in the tweet
    emoji_pattern = re.compile("["
                                    u"\U0001F600-\U0001F64F"  # emoticons
                                    u"\U0001F300-\U0001F5FF"  # symbols & pictographs
                                    u"\U0001F680-\U0001F6FF"  # transport & map symbols
                                    u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
                                    u"\U00002702-\U000027B0"
                                    u"\U000024C2-\U0001F251"
                                    u"\U0001F90C-\U0001F93A"  # Supplemental Symbols
                                    u"\U0001F93C-\U0001F945"  # and
                                    u"\U0001F947-\U0001F9FF"  # Pictographs
                                "]")
    emoji_count = len(emoji_pattern.findall(tweet))

    #Count the number of consecutive duplicate letters(len>2)
    consec_dup = len(re.findall(r'(\S)(\1{2,})', tweet))


    # Define a dictionary of features
    features = {
        'stop_word_count': stop_word_count,
        'exclamation_count': exclamation_count,
        'question_count': question_count,
        'top_bigrams': top_bigrams,
        'top_trigrams': top_trigrams,
        'emoji_count': emoji_count,
        'consecutive_duplicates': consec_dup
    }

    return features

def format_batch_texts(language_code, batch_texts):
    formated_bach = [">>{}<< {}".format(language_code, text) for text in batch_texts]

    return formated_bach

def perform_translation(batch_texts, model, tokenizer, language="en"):
    # Prepare the text data into appropriate format for the model
    formated_batch_texts = format_batch_texts(language, batch_texts)
    
    # Generate translation using model
    translated = model.generate(**tokenizer(formated_batch_texts, return_tensors="pt", padding=True))

    # Convert the generated tokens indices back into text
    translated_texts = [tokenizer.decode(t, skip_special_tokens=True) for t in translated]
    
    return translated_texts

def cleanData(dataset):
        
    for tweet in tweetColumn.values:
        #standard tweet cleaning
        clean = re.sub(r"(http[s]?\://\S+)|([\[\(].*[\)\]])|([#@]\S+)|\n", "", *tweet)

        #Test to see if they're useful or not
        clean = remove_emojis(clean)
        clean = removeConsecutiveDuplicates(clean)

        #mandatory arabic preprocessing
        clean = Normalize.normalize_searchtext(clean)
        clean = removeEnglish(clean)
        clean = lemmatizeArabic(clean)
        clean = removeStopwords(clean)
        clean = removePunctuation(clean)
        clean = tokenizeArabic(clean)

        #add to clean array
        cleaned_tweets.append(clean)

    return cleaned_tweets

def labelData(dataset):
    
    sarcasmColumn = dataset[["sarcasm"]]

    for sarcasm in sarcasmColumn.values:
        if sarcasm == True: labeled_column.append(1)
        else: labeled_column.append(0)

    return labeled_column

def prepared_dataset(tweets, sarcasm):
    data = pd.DataFrame({"Tweet": tweets, "Sarcasm": sarcasm})
    """
    This is for the feature extraction.

    preparedDataset(tweets, sarcasm, numEmojis, numSarcasticPunct, stopWordsCount, numDuplicateLetters):
    data = pd.DataFrame({"Tweet": tweets, "Sarcasm": sarcasm, "Emoji Count": numEmojis, "Punctuation Count": numSarcasticPunct, "Stop Words Count": stopWordsCount, "Duplicate Letter Count": numDuplicateLetters})
    """
    return data

def preProcessData(dataset):
    labeled = labelData(dataset)
    cleaned = cleanData(dataset)
    data = prepared_dataset(cleaned, labeled)

    df = dataset[['tweet']]#replace with cleaned dataset with just the features we want. Add a feature reduction function and then run the cleaning one
    features = df['tweet'].apply(extractSarcasticFeatures)
    feature_columns = features.apply(pd.Series).add_prefix('sarcasm_')
    df = pd.concat([data, feature_columns], axis=1)
    
    return df

#empty arrays before running function
cleaned_tweets = []
labeled_column = []

englishModelName = "Helsinki-NLP/opus-mt-ar-en"
frenchModelName = "Helsinki-NLP/opus-mt-en-fr"
arabicModelName = "Helsinki-NLP/opus-mt-fr-ar"

englishModeltkn = MarianTokenizer.from_pretrained(englishModelName)
frenchModeltkn = MarianTokenizer.from_pretrained(frenchModelName)
arabicModeltkn = MarianTokenizer.from_pretrained(arabicModelName)

englishModel = MarianMTModel.from_pretrained(englishModelName)
frenchModel = MarianMTModel.from_pretrained(frenchModelName)
arabicModel = MarianMTModel.from_pretrained(arabicModelName)

cleaned_dataset = preProcessData(dataset)
cleaned_dataset.head()
cleaned_dataset.to_csv('cleanedset.csv', encoding='utf-8')

files.download('cleanedset.csv')

cleaned_dataset.head()

dataset

"""--------------------------Below here are various tests--------------------------

"""

print(lemmatizeArabic("ينتظرون"))

print(tokenizeArabic(lemmatizeArabic(removeConsecutiveDuplicates(Normalize.normalize_searchtext(" هههههههههههههههههههه إحنا بنكتب صح يا باشا مش مستنين شوية إصفار ووحايد يقولولنا نكتب إزاي")))))

tokens = tokenizeArabic("الكويس إنك مش هتروح تبحث علي النت عشان تسطب اللغه")
print(tokens)
tagged = nltk.pos_tag(tokens) # tagger not accurate for arabic. Check other taggers
print(tagged)

print("أستشتري دمـــى آلية لأبنائك قبل الإغلاق"[::-1])

print(removeEnglish(removeStopwords("كان helloاحمد في المدرسة قبل البارحة")))

extractSarcasticFeatures(removePunctuation(removeEnglish(removeStopwords(re.sub(r"(http[s]?\://\S+)|([\[\(].*[\)\]])|([#@]\S+)|\n", "",dataset["tweet"][3])))))

test = [
        "د.محمود_العلايلي:أرى أن الفريق أحمد شفيق رقم مهم في المعادلة السياسية المصرية ولا يمكن إغفاله هل ترى أن هذا صحيح؟",
        "مع فيدرر يا آجا والكبار",
        "الداعون لمبدأ الاختلاط بين الجنسين؛ كالداعين لإلغاء التسعيرة كلاهما يريد تصفية السوق السوداء بجعلها حرة.",
        "قل شرق حلب ولا تقل حلب الشرقية ....وقل غرب حلب ولا تقل حلب الغربية ....فحلب موحدة ويريدونها منقسمة",
        "مرسي : مش هنام اكتر من اربع ساعات في اليوم...... لو نمت دقيقة كمان زيادة هديك علي دماغك",
        "ذهبت إلي المدرسة صباحا"
        ]

for tweet in test:

    englishVersion = perform_translation([tweet], englishModel, englishModeltkn, "en")
    frenchVersion = perform_translation(englishVersion, frenchModel, frenchModeltkn, "fr")
    arabicVersion = perform_translation(frenchVersion, arabicModel, arabicModeltkn, "ar")

    print(tweet)
    print(englishVersion)
    print(frenchVersion)
    print(arabicVersion)

    print("")