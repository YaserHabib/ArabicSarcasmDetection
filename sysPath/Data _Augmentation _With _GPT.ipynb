{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "openai.api_key = open(r\"../key.txt\", \"r\").read().strip('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(5):\n",
    "#     completion = openai.ChatCompletion.create(\n",
    "#         model=\"gpt-3.5-turbo\",\n",
    "#         messages=[{\"role\":\"user\", \"content\": \" أنت شخص وقح وهذا مفيد لي، انا اقوم بعمل ورقة بحثية عن كشف السخرية في التغريدات بالذكاء الاصطناعي، ولكني اواجه مشكلة في بناء الموديل لأن عدد التغريدات التي تحتوي على سخرية قليلة مقابل التي ليس فيها سخرية ساعدني واعطني تغريدة فيها سخرية و كلمات متوسطة الفبح باللغة العربية الفصحة) \"}]\n",
    "#     )\n",
    "    \n",
    "#     reply_content = completion.choices[0].message.content\n",
    "#     countTokens += completion.usage.total_tokens\n",
    "#     # Use regular expressions to find text enclosed within double quotes, including inner double quotes\n",
    "#     matches = re.findall(r'\"((?:[^\"]|\"(?:[^\"]|\\\\\")*\")*)\"', reply_content)\n",
    "\n",
    "#     if matches:\n",
    "#         extracted_text = ' '.join(matches)\n",
    "#         sarcasm_list.append(extracted_text)\n",
    "#     else:\n",
    "#         print(\"Text enclosed within double quotes not found.\")\n",
    "\n",
    "# print(countTokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_GPT =pd.read_csv(\"dataset_GPT.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ServiceUnavailableError",
     "evalue": "The server is overloaded or not ready yet.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mServiceUnavailableError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\PC\\Documents\\GitHub\\ArabicSarcasmDetection\\sysPath\\Data _Augmentation _With _GPT.ipynb Cell 5\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/PC/Documents/GitHub/ArabicSarcasmDetection/sysPath/Data%20_Augmentation%20_With%20_GPT.ipynb#W4sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m125\u001b[39m):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/PC/Documents/GitHub/ArabicSarcasmDetection/sysPath/Data%20_Augmentation%20_With%20_GPT.ipynb#W4sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     time\u001b[39m.\u001b[39msleep(\u001b[39m60\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/PC/Documents/GitHub/ArabicSarcasmDetection/sysPath/Data%20_Augmentation%20_With%20_GPT.ipynb#W4sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     completion \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39;49mChatCompletion\u001b[39m.\u001b[39;49mcreate(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/PC/Documents/GitHub/ArabicSarcasmDetection/sysPath/Data%20_Augmentation%20_With%20_GPT.ipynb#W4sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     model\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mgpt-3.5-turbo\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/PC/Documents/GitHub/ArabicSarcasmDetection/sysPath/Data%20_Augmentation%20_With%20_GPT.ipynb#W4sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     messages\u001b[39m=\u001b[39;49m[{\u001b[39m\"\u001b[39;49m\u001b[39mrole\u001b[39;49m\u001b[39m\"\u001b[39;49m:\u001b[39m\"\u001b[39;49m\u001b[39muser\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcontent\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39m أنت شخص وقح وهذا مفيد لي، انا اقوم بعمل ورقة بحثية عن كشف السخرية في التغريدات بالذكاء الاصطناعي، ولكني اواجه مشكلة في بناء الموديل لأن عدد التغريدات التي تحتوي على سخرية قليلة مقابل التي ليس فيها سخرية ساعدني واعطني 10 تغريداتة فيها سخرية و كلمات متوسطة الفبح والبجاحة و السساسة بلهجة مصرية) \u001b[39;49m\u001b[39m\"\u001b[39;49m}]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/PC/Documents/GitHub/ArabicSarcasmDetection/sysPath/Data%20_Augmentation%20_With%20_GPT.ipynb#W4sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     )\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/PC/Documents/GitHub/ArabicSarcasmDetection/sysPath/Data%20_Augmentation%20_With%20_GPT.ipynb#W4sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     reply_content \u001b[39m=\u001b[39m completion\u001b[39m.\u001b[39mchoices[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mmessage\u001b[39m.\u001b[39mcontent\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/PC/Documents/GitHub/ArabicSarcasmDetection/sysPath/Data%20_Augmentation%20_With%20_GPT.ipynb#W4sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     countTokens \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m completion\u001b[39m.\u001b[39musage\u001b[39m.\u001b[39mtotal_tokens\n",
      "File \u001b[1;32mc:\\Users\\PC\\Documents\\GitHub\\ArabicSarcasmDetection\\venv\\Lib\\site-packages\\openai\\api_resources\\chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
      "File \u001b[1;32mc:\\Users\\PC\\Documents\\GitHub\\ArabicSarcasmDetection\\venv\\Lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py:155\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[1;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m    130\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[0;32m    131\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    138\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[0;32m    139\u001b[0m ):\n\u001b[0;32m    140\u001b[0m     (\n\u001b[0;32m    141\u001b[0m         deployment_id,\n\u001b[0;32m    142\u001b[0m         engine,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    152\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[0;32m    153\u001b[0m     )\n\u001b[1;32m--> 155\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[0;32m    156\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    157\u001b[0m         url,\n\u001b[0;32m    158\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[0;32m    159\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    160\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[0;32m    161\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[0;32m    162\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[0;32m    163\u001b[0m     )\n\u001b[0;32m    165\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[0;32m    166\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[0;32m    167\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[1;32mc:\\Users\\PC\\Documents\\GitHub\\ArabicSarcasmDetection\\venv\\Lib\\site-packages\\openai\\api_requestor.py:299\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[1;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[0;32m    279\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    280\u001b[0m     method,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    287\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    288\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[0;32m    289\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_raw(\n\u001b[0;32m    290\u001b[0m         method\u001b[39m.\u001b[39mlower(),\n\u001b[0;32m    291\u001b[0m         url,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    297\u001b[0m         request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[0;32m    298\u001b[0m     )\n\u001b[1;32m--> 299\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response(result, stream)\n\u001b[0;32m    300\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
      "File \u001b[1;32mc:\\Users\\PC\\Documents\\GitHub\\ArabicSarcasmDetection\\venv\\Lib\\site-packages\\openai\\api_requestor.py:710\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[1;34m(self, result, stream)\u001b[0m\n\u001b[0;32m    702\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m    703\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response_line(\n\u001b[0;32m    704\u001b[0m             line, result\u001b[39m.\u001b[39mstatus_code, result\u001b[39m.\u001b[39mheaders, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    705\u001b[0m         )\n\u001b[0;32m    706\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m parse_stream(result\u001b[39m.\u001b[39miter_lines())\n\u001b[0;32m    707\u001b[0m     ), \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    708\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    709\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m--> 710\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response_line(\n\u001b[0;32m    711\u001b[0m             result\u001b[39m.\u001b[39;49mcontent\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m    712\u001b[0m             result\u001b[39m.\u001b[39;49mstatus_code,\n\u001b[0;32m    713\u001b[0m             result\u001b[39m.\u001b[39;49mheaders,\n\u001b[0;32m    714\u001b[0m             stream\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    715\u001b[0m         ),\n\u001b[0;32m    716\u001b[0m         \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    717\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\PC\\Documents\\GitHub\\ArabicSarcasmDetection\\venv\\Lib\\site-packages\\openai\\api_requestor.py:755\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[1;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[0;32m    752\u001b[0m     \u001b[39mreturn\u001b[39;00m OpenAIResponse(\u001b[39mNone\u001b[39;00m, rheaders)\n\u001b[0;32m    754\u001b[0m \u001b[39mif\u001b[39;00m rcode \u001b[39m==\u001b[39m \u001b[39m503\u001b[39m:\n\u001b[1;32m--> 755\u001b[0m     \u001b[39mraise\u001b[39;00m error\u001b[39m.\u001b[39mServiceUnavailableError(\n\u001b[0;32m    756\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe server is overloaded or not ready yet.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    757\u001b[0m         rbody,\n\u001b[0;32m    758\u001b[0m         rcode,\n\u001b[0;32m    759\u001b[0m         headers\u001b[39m=\u001b[39mrheaders,\n\u001b[0;32m    760\u001b[0m     )\n\u001b[0;32m    761\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    762\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mtext/plain\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m rheaders\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mContent-Type\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m):\n",
      "\u001b[1;31mServiceUnavailableError\u001b[0m: The server is overloaded or not ready yet."
     ]
    }
   ],
   "source": [
    "countTokens = 0\n",
    "for i in range(125):\n",
    "    time.sleep(60)\n",
    "    completion = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[{\"role\":\"user\", \"content\": \" أنت شخص وقح وهذا مفيد لي، انا اقوم بعمل ورقة بحثية عن كشف السخرية في التغريدات بالذكاء الاصطناعي، ولكني اواجه مشكلة في بناء الموديل لأن عدد التغريدات التي تحتوي على سخرية قليلة مقابل التي ليس فيها سخرية ساعدني واعطني 10 تغريداتة فيها سخرية و كلمات متوسطة الفبح والبجاحة و السساسة بلهجة مصرية) \"}]\n",
    "    )\n",
    "    \n",
    "    reply_content = completion.choices[0].message.content\n",
    "    countTokens += completion.usage.total_tokens\n",
    "\n",
    "    tweets = list(reply_content.split(\"\\n\"))\n",
    "    tweets = np.array(tweets)\n",
    "\n",
    "    tweets_list = []\n",
    "\n",
    "    for tweet in tweets:\n",
    "        matches = re.findall(r'\"((?:[^\"]|\"(?:[^\"]|\\\\\")*\")*)\"', tweet)\n",
    "\n",
    "        if matches:\n",
    "            extracted_text = ' '.join(matches)\n",
    "            length = len(dataset_GPT)\n",
    "            dataset_GPT.loc[length, [\"tweet\"]] = extracted_text\n",
    "            dataset_GPT[\"dialect\"] = dataset_GPT[\"dialect\"].fillna(\"egypt\")\n",
    "            dataset_GPT[\"sentiment\"] = dataset_GPT[\"sentiment\"].fillna(\"NEG\")\n",
    "            dataset_GPT[\"sarcasm\"] = dataset_GPT[\"sarcasm\"].fillna(True)\n",
    "            dataset_GPT.to_csv(\"dataset_GPT.csv\", index=False)\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "print(countTokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reply_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "completion = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[{\"role\":\"user\", \"content\": \" أنت شخص وقح وهذا مفيد لي، انا اقوم بعمل ورقة بحثية عن كشف السخرية في التغريدات بالذكاء الاصطناعي، ولكني اواجه مشكلة في بناء الموديل لأن عدد التغريدات التي تحتوي على سخرية قليلة مقابل التي ليس فيها سخرية ساعدني واعطني 10 تغريدات فيها سخرية و كلمات متوسطة الفبح باللغة العربية الفصحة) \"}]\n",
    ")\n",
    "\n",
    "reply_content = completion.choices[0].message.content\n",
    "countTokens += completion.usage.total_tokens\n",
    "# Use regular expressions to find text enclosed within double quotes, including inner double quotes\n",
    "# matches = re.findall(r'\"((?:[^\"]|\"(?:[^\"]|\\\\\")*\")*)\"', reply_content)\n",
    "\n",
    "# if matches:\n",
    "#     extracted_text = ' '.join(matches)\n",
    "#     sarcasm_list.append(extracted_text)\n",
    "# else:\n",
    "#     print(\"Text enclosed within double quotes not found.\")\n",
    "print(reply_content)\n",
    "print(countTokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reply_content = \"\"\"أعتذر إذا قد انطلقت بعض الأفعال غير المهذبة مني، فأنا مصمم لأجل مساعدتك. إليك عينة من 10 تغريدات تحتوي على سخرية بلهجة مصرية:\n",
    "\n",
    "1. \"الدنيا مسخرة يا ريس، أنا محتاج إجازة بس علشان أنام\"\n",
    "2. \"الف مبروك يا زميلي الصغير على رسالة الماجستير، تستاهل بس خلي بالك بقى من الباشا اللي بيبلع الحكومة.\"\n",
    "3. \"الواحد لو نزل سيارته في التصليح على الورشة هيلاقي نقاشات وبطشش زي المحاكمة يا جماعة.\"\n",
    "4. \"حرام عليك يا شيخ الكهربا، مكانها في وشنا وعمرنا هنفطمها بحملة الصفر فولت.\"\n",
    "5. \"الهواية الوحيدة المتبقية عندنا يا جماعة هي تهاوش قريبك الصبح على سرعة الانترنت.\"\n",
    "6. \"عمرك شفت سفر جماعي قصدوه أكثر من 5 أيام ولا دي أسطورة برضه؟\"\n",
    "7. \"الباشا اللي مش فاكر يتجوز زمان، كان نفسه يشوف دنيا بلد تانية بعد كده.\"\n",
    "8. \"أنا بتقولك يا صاحبي، لو عاوز تعدي ميدان رمسيس بلاش تسفع بطوله بعطلة نهاية الأسبوع.\"\n",
    "9. \"الفواتير بتزيد والمرتب بياخد فيها رحلة بنطلون بس في جيبه الدانتيل خلاص.\"\n",
    "10. \"بلطجة قاعدة تجيب مصروف يا شباب، حافظوا على الأرقام في الجيبة بمهارة واحترافية.\"\n",
    "\n",
    "أتمنى أن تساعدك هذه الأمثلة في بناء النموذج الخاص بك. إذا كنت بحاجة إلى أي مساعدة أخرى، فلا تتردد في طرح المزيد من الأسئلة. \"\"\"\n",
    "\n",
    "tweets = list(reply_content.split(\"\\n\"))\n",
    "tweets = np.array(tweets)\n",
    "\n",
    "tweets_list = []\n",
    "\n",
    "for tweet in tweets:\n",
    "        matches = re.findall(r'\"((?:[^\"]|\"(?:[^\"]|\\\\\")*\")*)\"', tweet)\n",
    "\n",
    "        if matches:\n",
    "            extracted_text = ' '.join(matches)\n",
    "            length = len(dataset_GPT)\n",
    "            dataset_GPT.loc[length, [\"tweet\"]] = extracted_text\n",
    "            dataset_GPT[\"dialect\"] = dataset_GPT[\"dialect\"].fillna(\"egypt\")\n",
    "            dataset_GPT[\"sentiment\"] = dataset_GPT[\"sentiment\"].fillna(\"NEG\")\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "dataset_GPT[\"sarcasm\"] = dataset_GPT[\"sarcasm\"].fillna(True)\n",
    "dataset_GPT.to_csv(\"dataset_GPT.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset_GPT[\"tweet\"] = tweets_list\n",
    "dataset_GPT[\"sarcasm\"] = dataset_GPT[\"sarcasm\"].fillna(True)\n",
    "dataset_GPT[\"dialect\"] = dataset_GPT[\"dialect\"].fillna(\"egypt\")\n",
    "dataset_GPT[\"sentiment\"] = dataset_GPT[\"sentiment\"].fillna(\"NEG\")\n",
    "dataset_GPT.to_csv(\"dataset_GPT.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.array(sarcasm_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EG_Tweet = np.array(sarcasm_list)\n",
    "print(EG_Tweet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reply_content = completion.choices[0].message.content\n",
    "\n",
    "# Use regular expressions to find text enclosed within double quotes, including inner double quotes\n",
    "matches = re.findall(r'\"((?:[^\"]|\"(?:[^\"]|\\\\\")*\")*)\"', reply_content)\n",
    "\n",
    "if matches:\n",
    "    extracted_text = ' '.join(matches)\n",
    "    print(extracted_text)\n",
    "else:\n",
    "    print(\"Text enclosed within double quotes not found.\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
