# -*- coding: utf-8 -*-
"""
Sarcasm_Model.ipynb
Automatically generated by Colaboratory.
Original file is located at
    https://colab.research.google.com/drive/1fA0Z9TXYuseG3lCsrfOx9_6yrY8luSDj
"""

import os
import sys
abspath = os.path.abspath(__file__)
dname = os.path.dirname(abspath)
sys.path.append(r"C:\Users\Mohamed\Documents\Fall 2023 - 2024\Senior Project in CS\sysPath")
os.chdir(dname)

import re
import nltk
import shutil
import warnings
import numpy as np
import pandas as pd
import seaborn as sns
import preProcessData #type: ignore
import tensorflow as tf
import matplotlib.pyplot as plt
import pyarabic.normalize as Normalize

from nltk import ngrams
from numpy import array
from nltk.corpus import stopwords
from nltk.stem import ISRIStemmer
from keras.utils import plot_model
from keras.utils import pad_sequences
from gensim.models import KeyedVectors
from nltk.tokenize import word_tokenize
from keras.callbacks import TensorBoard
from keras.preprocessing.text import Tokenizer
from transformers import MarianMTModel, MarianTokenizer

from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, precision_score, recall_score, f1_score

# nltk.download('punkt')  # download punkt tokenizer if not already downloaded
# nltk.download('stopwords') # download stopwords if not already downloaded
# nltk.download('averaged_perceptron_tagger')

warnings.filterwarnings(action = 'ignore')
callback = TensorBoard(log_dir='logs/', histogram_freq=1)

if os.path.isdir("logs"):
    shutil.rmtree("logs")


# dataset = pd.read_csv(r"https://raw.githubusercontent.com/iabufarha/ArSarcasm-v2/main/ArSarcasm-v2/training_data.csv")
# dataset = pd.read_csv(r"C:\Users\Mohamed\Documents\Fall 2023 - 2024\Senior Project in CS\Datasets\GPT Dataset.csv")
dataset = pd.read_csv(r"C:\Users\Mohamed\Documents\Fall 2023 - 2024\Senior Project in CS\Datasets\full Dataset.csv")
dataset.info()
print(f"\n{dataset.head()}")
cleaned_dataset = preProcessData.preProcessData(dataset.copy(deep=True))



# prepare tokenizer
T = Tokenizer()
T.fit_on_texts(cleaned_dataset["tweet"].tolist())
vocab_size = len(T.word_index) + 1



# integer encode the documents
encoded_docs = T.texts_to_sequences(cleaned_dataset["tweet"].tolist())
# print("encoded_docs:\n",encoded_docs)



# pad documents to a max length of 4 words
max_length = len(max(np.array(dataset["tweet"]), key=len))
padded_docs = pad_sequences(encoded_docs, maxlen = max_length, padding = "post")
print("\npadded_docs:\n",padded_docs)



# load the whole embedding into memory
w2v_embeddings_index = {}
TOTAL_EMBEDDING_DIM = 300
# Aravec Twitter-CBOW Model ===> https://bakrianoo.ewr1.vultrobjects.com/aravec/full_grams_cbow_300_twitter.zip
# Aravec Twitter-SkipGram Model ===> https://bakrianoo.ewr1.vultrobjects.com/aravec/full_grams_sg_300_twitter.zip
# Aravec Wikipedia-CBOW Model ===> https://bakrianoo.ewr1.vultrobjects.com/aravec/full_grams_cbow_300_wiki.zip
# Aravec Wikipedia-SkipGram Model ===> https://bakrianoo.ewr1.vultrobjects.com/aravec/full_grams_sg_300_wiki.zip
embeddings_file = r"C:\Users\Mohamed\Documents\Fall 2023 - 2024\Senior Project in CS\full_grams_cbow_300_twitter\full_grams_cbow_300_twitter.mdl"
w2v_model = KeyedVectors.load(embeddings_file)



for word in w2v_model.wv.index_to_key:
    w2v_embeddings_index[word] = w2v_model.wv[word]

print("\nLoaded %s word vectors."% len(w2v_embeddings_index))



# create a weight matrix for words in training docs
embedding_matrix = np.zeros((vocab_size, TOTAL_EMBEDDING_DIM))

for word, i in T.word_index.items():
    embedding_vector = w2v_embeddings_index.get(word)
    if embedding_vector is not None:
        embedding_matrix[i] = embedding_vector

print("\nEmbedding Matrix shape:", embedding_matrix.shape)



# define model
# embedding_layer = tf.keras.layers.Embedding(vocab_size, TOTAL_EMBEDDING_DIM, weights=[embedding_matrix], input_length=4, trainable=False)
# input_placeholder= tf.keras.Input(shape=(max_length,), dtype = "int32")
# input_embedding = embedding_layer(input_placeholder)
# lstm = tf.keras.layers.LSTM(units = 10, activation = "relu")(input_embedding)
# preds = tf.keras.layers.Dense(1, activation = "sigmoid", name = "activation")(lstm)
# model = tf.keras.models.Model(inputs = input_placeholder, outputs = preds)

model = tf.keras.Sequential([
    # Embedding layer for creating word embeddings
    tf.keras.layers.Embedding(vocab_size, TOTAL_EMBEDDING_DIM, input_length=max_length),

    # GlobalMaxPooling layer to extract relevant features
    tf.keras.layers.GlobalMaxPool1D(),

    # First Dense layer with 40 neurons and ReLU activation
    tf.keras.layers.Dense(40, activation='relu'),

    # Dropout layer to prevent overfitting
    tf.keras.layers.Dropout(0.5),

    # Second Dense layer with 20 neurons and ReLU activation
    tf.keras.layers.Dense(20, activation='relu'),

    # Dropout layer to prevent overfitting
    tf.keras.layers.Dropout(0.5),

    # Third Dense layer with 10 neurons and ReLU activation
    tf.keras.layers.Dense(10, activation='relu'),

    # Dropout layer to prevent overfitting
    tf.keras.layers.Dropout(0.2),

    # Final Dense layer with 1 neuron and sigmoid activation for binary classification
    tf.keras.layers.Dense(1, activation='sigmoid')
])


# compile the model
model.compile(loss="binary_crossentropy", optimizer=tf.keras.optimizers.Adam(learning_rate = 0.001, beta_1=0.9, beta_2=0.999), metrics=["accuracy"])

# summarize the model
print(f"\n{model.summary()}")
print("\n # Wait just Fitting model on training data")
plot_model(model, to_file='summary.png', show_shapes=True, show_layer_names=True, dpi=1000)

# splits into traint & test
tweet_train, tweet_test, labeled_train, labeled_test = train_test_split(padded_docs, array(cleaned_dataset[["sarcasm"]].values.tolist()), test_size=0.20, random_state=42)

# fit the model
model.fit(tweet_train, labeled_train, epochs = 5, verbose = 1, callbacks=[callback]) # type: ignore

# evaluate the model
loss, accuracy = model.evaluate(tweet_test, labeled_test, verbose = 0) # type: ignore
print("\nAccuracy: %f" %(accuracy*100))




# get Classification report
predicted = np.round(model.predict(tweet_test))
report = classification_report(labeled_test, predicted, target_names=["0", "1"])

print(f"\n{report}\n")



confusionMatrix = confusion_matrix(labeled_test, predicted)

ax= plt.subplot()
sns.heatmap(confusionMatrix, annot=True, fmt='g', ax=ax, cmap="viridis") # annot=True to annotate cells, ftm='g' to disable scientific notation

# labels, title and ticks
ax.set_xlabel('Predicted labels')
ax.set_ylabel('True labels')
ax.set_title(f"Accuracy: {accuracy*100:.2f}%")
ax.xaxis.set_ticklabels(["0", "1"])
ax.yaxis.set_ticklabels(["0", "1"])

plt.savefig(f"keras - Full dataset.png", dpi=1000)
plt.close()